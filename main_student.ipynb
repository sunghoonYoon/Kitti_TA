{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/sunghoonYoon/Kitti_TA\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "####NOTE: you should change the \"runtime option to use GPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kitti_options = NONE #SS: Semantic Segmentation , SM: Stereo Matching, OF: Optical flow /// Only these three options are available\n",
    "\n",
    "# assert Kitti_options in ['SS','SM','OF']\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from imageio import imread, imwrite\n",
    "\n",
    "########\n",
    "from Kitti_TA.kitti_seg import decode_segmap, denorm, YOUR_IMPLEMENTATION ##import segmentation network (deeplabWplus)\n",
    "from Kitti_TA.kitti_stereo import YOUR_IMPLEMENTATION #import stereo matching network\n",
    "from Kitti_TA.kitti_flow import ArrayToTensor, flow2rgb, YOUR_IMPLEMENTATION #import stereo matching network\n",
    "\n",
    "\n",
    "left0_image_path = '/content/drive/MyDrive/TA_stereo/2011_09_26_drive_0096_sync/image_02/data/0000000000.png'\n",
    "right0_image_path = '/content/drive/MyDrive/TA_stereo/2011_09_26_drive_0096_sync/image_03/data/0000000000.png'\n",
    "\n",
    "left1_image_path = '/content/drive/MyDrive/TA_stereo/2011_09_26_drive_0096_sync/image_02/data/0000000001.png'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net_SS = YOUR_IMPLEMENTATION.cuda() # declare Network and the number of class=19\n",
    "state_dict= torch.load('/content/drive/MyDrive/TA_segmentation/cityscapes_best.pth')\n",
    "net_SS = torch.nn.DataParallel(net_SS)\n",
    "    \n",
    "net_SS.YOUR_IMPLEMENTATION(state_dict['state_dict'],strict=False) #load the checkpoint(state_dict) \n",
    "\n",
    "net_SS.eval()\n",
    "\n",
    "image= np.asarray(Image.open(left0_image_path)).convert('RGB')\n",
    "\n",
    "MEAN = [0.45734706, 0.43338275, 0.40058118]\n",
    "STD = [0.23965294, 0.23532275, 0.2398498]\n",
    "\n",
    "toTensor = transforms.ToTensor()\n",
    "normTensor = transforms.Normalize(MEAN,STD)\n",
    "imageT = normTensor(toTensor(image))\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    imageT = F.interpolate(imageT.unsqueeze(0), size=(512,1024),mode='bilinear',align_corners=True)\n",
    "    YOUR_IMPLEMENTATION # get segmentation results using network\n",
    "\n",
    "    YOUR_IMPLEMENTATION # Take softmax to the network output in channel dimension\n",
    "    YOUR_IMPLEMENTATION # Get max indices from the softmax-ed result (use argmax function)\n",
    "\n",
    "segmap = YOUR_IMPLEMENTATION #use the function that we provide (convert prediction into RBG color map)\n",
    "\n",
    "###Visualization###\n",
    "'''\n",
    "YOUR IMPLEMENTATION\n",
    "#Visualize the segmentation result and image together with transparency (alpha=0.7) mode. use plt function\n",
    "\n",
    "''' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stereo matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_disp = 192\n",
    "net_SM = YOUR_IMPLEMENTATION # declare Network and the max disparity is 192\n",
    "net_SM = net_SM.cuda()\n",
    "net_SM.eval()\n",
    "\n",
    "checkpoint = torch.load('/content/drive/MyDrive/TA_stereo/pretrained_model_KITTI2015.tar')\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in checkpoint['state_dict'].items():\n",
    "    name = k[7:]\n",
    "    new_state_dict[name] = v    \n",
    "net_SM.load_state_dict(new_state_dict)    \n",
    "normal_mean_var = {'mean': [0.485, 0.456, 0.406],\n",
    "                    'std': [0.229, 0.224, 0.225]}\n",
    "infer_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize(**normal_mean_var)])    \n",
    "\n",
    "\n",
    "imgL_o = Image.open(left0_image_path).convert('RGB')\n",
    "imgR_o = Image.open(right0_image_path).convert('RGB')\n",
    "\n",
    "imgL = infer_transform(imgL_o)\n",
    "imgR = infer_transform(imgR_o) \n",
    "\n",
    "# pad to width and hight to 16 times\n",
    "if imgL.shape[1] % 16 != 0:\n",
    "    times = imgL.shape[1]//16       \n",
    "    top_pad = (times+1)*16 -imgL.shape[1]\n",
    "else:\n",
    "    top_pad = 0\n",
    "\n",
    "if imgL.shape[2] % 16 != 0:\n",
    "    times = imgL.shape[2]//16                       \n",
    "    right_pad = (times+1)*16-imgL.shape[2]\n",
    "else:\n",
    "    right_pad = 0    \n",
    "\n",
    "imgL = F.pad(imgL,(0,right_pad, top_pad,0)).unsqueeze(0)\n",
    "imgR = F.pad(imgR,(0,right_pad, top_pad,0)).unsqueeze(0)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "net_SM.eval()\n",
    "\n",
    "imgL = imgL.cuda()\n",
    "imgR = imgR.cuda()  \n",
    "        \n",
    "\n",
    "with torch.no_grad():\n",
    "    disp = YOUR_IMPLEMENTATION # get output from the network, input: (imgL,imgR)\n",
    "\n",
    "    disp = torch.squeeze(disp)\n",
    "    pred_disp = disp.data.cpu().numpy()\n",
    "\n",
    "\n",
    "if top_pad !=0 or right_pad != 0:\n",
    "    img = pred_disp[top_pad:,:-right_pad]\n",
    "else:\n",
    "    img = pred_disp\n",
    "\n",
    "####Visualize####\n",
    "'''\n",
    "YOUR IMPLEMENTATION\n",
    "#Visualize the left image and left disparity using plt function.\n",
    "\n",
    "'''\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optical flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('/content/drive/MyDrive/TA_opticalflow/flownets_bn_EPE2.459.pth.tar')\n",
    "\n",
    "net_OF = YOUR_IMPLEMENTATION # declare Network for optical flow\n",
    "net_OF = net_OF.cuda()\n",
    "net_OF.eval()\n",
    "\n",
    "input_transform = transforms.Compose([\n",
    "    ArrayToTensor(),\n",
    "    transforms.Normalize(mean=[0,0,0], std=[255,255,255]),\n",
    "    transforms.Normalize(mean=[0.411,0.432,0.45], std=[1,1,1])\n",
    "])\n",
    "\n",
    "\n",
    "img1= input_transform(imread(left0_image_path))\n",
    "img2= input_transform(imread(left1_image_path))\n",
    "\n",
    "input_var = YOUR_IMPLEMENTATION # concatenate \"img1\" and \"img2\" and expand zero dimentions so that the dimension (1,C,H,W)\n",
    "\n",
    "net_OF.load_state_dict(state_dict['state_dict'])\n",
    "\n",
    "bidirectional = True\n",
    "if bidirectional:\n",
    "    # feed inverted pair along with normal pair\n",
    "    inverted_input_var = YOUR_IMPLEMENTATION # concatenate \"img2\" and \"img1\" and expand zero dimentions so that the dimension (1,C,H,W), Note that the order (img2,img1) is important\n",
    "    input_var = torch.cat([input_var, inverted_input_var])\n",
    "\n",
    "input_var = input_var.cuda()\n",
    "# compute output\n",
    "flow_output = YOUR_IMPLEMENTATION #Get optical flow network output (input: input_var)\n",
    "\n",
    "rgb_flow = flow2rgb(20 * flow_output[0], max_value=20)\n",
    "rgb_inv_flow = flow2rgb(20 * flow_output[1], max_value=20)\n",
    "to_save = (rgb_flow * 255).astype(np.uint8).transpose(1,2,0)\n",
    "to_save_inv = (rgb_inv_flow * 255).astype(np.uint8).transpose(1,2,0)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('flow result')\n",
    "plt.imshow(to_save)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('inv flow result')\n",
    "plt.imshow(to_save_inv)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = '/content/drive/MyDrive/TA_stereo/2011_09_26_drive_0096_sync/image_02/data/*.png'\n",
    "save_dir = '/content/drive/MyDrive/TA_segmentation/output'\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "files = glob.glob(file_dir)\n",
    "\n",
    "'''\n",
    "This is manual (incompleted code) for you to convert output into video.\n",
    "1) extract the output and save it to png.\n",
    "2) used cv2.VideoWriter function to make video from png image files\n",
    "3) Make Video for all 3 tasks (Semantic Segmetnation /Stereo Matching/ Optical Flow)\n",
    "\n",
    "'''\n",
    "\n",
    "for file in files:\n",
    "  with torch.no_grad():\n",
    "    image = np.asarray(Image.open(file).convert('RGB'))\n",
    "    imageT = normTensor(toTensor(image))\n",
    "    imageT = F.interpolate(imageT.unsqueeze(0), size=(512,1024),mode='bilinear',align_corners=True)\n",
    "    out = net(imageT.cuda())\n",
    "\n",
    "    '''YOUR IMPLEMENTATION''' #Get output \n",
    "\n",
    "\n",
    "    plt.savefig(os.path.join(save_dir,os.path.basename(file)))\n",
    "    plt.close()\n",
    "\n",
    "### Convert image to video\n",
    "\n",
    "print(save_dir+'segmentation_video.avi') #change name if needed\n",
    "out = cv2.VideoWriter(save_dir+\"/\"+'segmentation_video.avi',cv2.VideoWriter_fourcc(*\"MJPG\"), 2, (1024,512)) #change name if needed\n",
    "\n",
    "for i in range(2):\n",
    "    img = cv2.imread(os.path.join('/content/drive/MyDrive/TA_segmentation/output',\"%010d.png\"%i))\n",
    "    print(img.shape)\n",
    "    out.write(img)\n",
    "\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
